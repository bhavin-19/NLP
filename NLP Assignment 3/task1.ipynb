{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pkMf6wtsESk"
      },
      "outputs": [],
      "source": [
        "## Add imports here\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_LEN = 50\n",
        "\n",
        "def initialise_projections(in_dim, out_dim):\n",
        "    \"\"\"\n",
        "    create projections for Q, K, V.\n",
        "    \"\"\"\n",
        "    return nn.Linear(in_dim, out_dim, bias=False)\n",
        "\n",
        "def pairwise_similarities(Q, K):\n",
        "    \"\"\"\n",
        "    Compute dot product attention.\n",
        "    \"\"\"\n",
        "    return torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "def attention_scaled(att_scores, d_k):\n",
        "    \"\"\"\n",
        "    Scale the raw attention scores.\n",
        "    \"\"\"\n",
        "    inv_sqrt = float(d_k) ** -0.5\n",
        "    return att_scores * inv_sqrt\n",
        "\n",
        "def attention_softmax(scaled_att_scores):\n",
        "    \"\"\"\n",
        "    Normalize the scaled raw attention scores with softmax.\n",
        "    \"\"\"\n",
        "    return F.softmax(scaled_att_scores, dim=-1)\n",
        "\n",
        "def compute_outputs(att_probs, V):\n",
        "    \"\"\"\n",
        "    Get outputs as a weighted sum of values by attention scores.\n",
        "    \"\"\"\n",
        "    return torch.matmul(att_probs, V)\n",
        "\n",
        "def make_causal_mask(size):\n",
        "    \"\"\"\n",
        "    Create a mask matrix that masks future context for the attention.\n",
        "    \"\"\"\n",
        "    return torch.tril(torch.ones(size, size)).to(DEVICE)\n",
        "\n",
        "def apply_causal_mask(att_scores, mask):\n",
        "    \"\"\"\n",
        "    Apply mask to attention.\n",
        "    \"\"\"\n",
        "    att_scores = att_scores.masked_fill(mask == 0, float('-inf'))\n",
        "    return att_scores\n",
        "\n",
        "def split_heads(x, num_heads):\n",
        "    \"\"\"\n",
        "    Splitting the input across multiple heads.\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, embed_dim = x.size()\n",
        "    head_dim = embed_dim // num_heads\n",
        "    x = x.view(batch_size, seq_len, num_heads, head_dim)\n",
        "    return x.transpose(1, 2)\n",
        "\n",
        "def merge_heads(x):\n",
        "    \"\"\"\n",
        "    Reversing splitting action of function split_heads().\n",
        "    \"\"\"\n",
        "    batch_size, num_heads, seq_len, head_dim = x.size()\n",
        "    x = x.transpose(1, 2).contiguous()\n",
        "    return x.view(batch_size, seq_len, num_heads * head_dim)\n",
        "\n",
        "def self_attention(x, projection_q, projection_k, projection_v, num_heads):\n",
        "    \"\"\"\n",
        "    Self-attention block.\n",
        "    \"\"\"\n",
        "    Q = projection_q(x)\n",
        "    K = projection_k(x)\n",
        "    V = projection_v(x)\n",
        "    Q = split_heads(Q, num_heads)\n",
        "    K = split_heads(K, num_heads)\n",
        "    V = split_heads(V, num_heads)\n",
        "    att_scores = pairwise_similarities(Q, K)\n",
        "    d_k = K.size(-1)\n",
        "    att_scores = attention_scaled(att_scores, d_k)\n",
        "    seq_len = x.size(1)\n",
        "    mask = make_causal_mask(seq_len)\n",
        "    mask = mask.unsqueeze(0).unsqueeze(1)\n",
        "    att_scores = apply_causal_mask(att_scores, mask)\n",
        "    att_probs = attention_softmax(att_scores)\n",
        "    att_output = compute_outputs(att_probs, V)\n",
        "    return merge_heads(att_output)\n",
        "\n",
        "def split_heads_qkv(Q, K, V, num_heads):\n",
        "    \"\"\"\n",
        "    Split Q, K, V across multiple heads.\n",
        "    \"\"\"\n",
        "    Q = split_heads(Q, num_heads)\n",
        "    K = split_heads(K, num_heads)\n",
        "    V = split_heads(V, num_heads)\n",
        "    return Q, K, V\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    # Read raw text splits\n",
        "    with open(\"/content/shakespear_train.txt\", \"r\") as f:\n",
        "        lines_train = f.read().splitlines()\n",
        "    with open(\"/content/shakespear_dev.txt\", \"r\") as f:\n",
        "        lines_dev = f.read().splitlines()\n",
        "    #with open(\"/content/shakespear_test.txt\", \"r\") as f:\n",
        "        #lines_test = f.readlines()\n",
        "\n",
        "\n",
        "    # Split each line into words for counting\n",
        "    tokens_train = [ln.split() for ln in lines_train]\n",
        "\n",
        "    # Utility function to flatten tokens\n",
        "    def flat(tokens):\n",
        "        flattened = []\n",
        "        for group in tokens:\n",
        "            flattened.extend(group)\n",
        "        return flattened\n",
        "\n",
        "    # Count frequency of each token in the training set\n",
        "    token_counts = Counter(flat(tokens_train))\n",
        "\n",
        "    # Create a tokenizer for a mapping and inverse mapping\n",
        "    vocab = [\"<PAD>\", \"<START>\", \"<STOP>\"] + sorted(token_counts.keys())\n",
        "    tokenizer = {token: idx for idx, token in enumerate(vocab)}\n",
        "    tokenizer_inv = {idx: token for token, idx in tokenizer.items()}\n",
        "\n",
        "    # Prepare datasets by converting each non-empty line into ID sequences\n",
        "    def tokenizer_line(line):\n",
        "        words = line.strip().split()\n",
        "        if not words:\n",
        "            return []\n",
        "        seq = [tokenizer[\"<START>\"]]\n",
        "        for w in words:\n",
        "            seq.append(tokenizer.get(w, tokenizer[\"<PAD>\"]))\n",
        "        seq.append(tokenizer[\"<STOP>\"])\n",
        "        return seq\n",
        "\n",
        "    data_train = [tokenizer_line(ln) for ln in lines_train if ln]\n",
        "    data_val   = [tokenizer_line(ln) for ln in lines_dev   if ln]\n",
        "\n",
        "    return data_train, data_val, tokenizer, tokenizer_inv\n",
        "\n",
        "\n",
        "def pad_to_length(tokens, max_len, tokenizer):\n",
        "    \"\"\"\n",
        "    Pad tokens to a fixed length.\n",
        "    \"\"\"\n",
        "    pad_token = tokenizer[\"<PAD>\"]\n",
        "    curr = len(tokens)\n",
        "    if curr < max_len:\n",
        "        num_pad = max_len - curr\n",
        "        tokens = tokens + [pad_token] * num_pad\n",
        "    elif curr > max_len:\n",
        "        tokens = tokens[:max_len]\n",
        "    return tokens\n",
        "\n",
        "def tokenize(sentence, pad_to_len=None, tokenizer=None, include_stop=True):\n",
        "    \"\"\"\n",
        "    Tokenize a sentence\n",
        "    \"\"\"\n",
        "    if isinstance(sentence, list):\n",
        "        tokens = sentence  # already token IDs\n",
        "    else:\n",
        "        words = sentence.strip().split()\n",
        "        tokens = [tokenizer.get(w, tokenizer[\"<PAD>\"]) for w in words]\n",
        "\n",
        "    if pad_to_len:\n",
        "        tokens = pad_to_length(tokens, pad_to_len, tokenizer)\n",
        "\n",
        "    # ensure we never exceed max\n",
        "    assert len(tokens) <= pad_to_len, \"tokenized length > pad_to_len\"\n",
        "    return tokens\n",
        "\n",
        "def decode(tokens, tokenizer_inv, end_at_stop=True, omit_pad=True):\n",
        "    \"\"\"\n",
        "    Decode tokens to text.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for t in tokens:\n",
        "        w = tokenizer_inv.get(t, \"\")\n",
        "        if omit_pad and w == \"<PAD>\":\n",
        "            continue\n",
        "        result.append(w)\n",
        "        if end_at_stop and w == \"<STOP>\":\n",
        "            break\n",
        "    else:\n",
        "        pass\n",
        "    return \" \".join(result)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_losses(data, model, tokenizer, bs=32, progress=True, pad_to_len=MAX_LEN):\n",
        "    it = range(0, len(data), bs)\n",
        "    if progress:\n",
        "        it = tqdm(it)\n",
        "\n",
        "    out = []\n",
        "    for b_start in it:\n",
        "        batch = slice(b_start, b_start + bs)\n",
        "        tokens = torch.tensor(\n",
        "            [tokenize(t, pad_to_len=pad_to_len, tokenizer=tokenizer) for t in data[batch]], dtype=torch.long\n",
        "        ).to(DEVICE)\n",
        "        X_tokens, y_tokens = tokens[:, :-1].contiguous(), tokens[:, 1:].contiguous()\n",
        "\n",
        "        model.eval()\n",
        "        logits, _ = model(X_tokens)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        y_log_probs = torch.gather(log_probs, 2, y_tokens[..., None])[..., 0]\n",
        "\n",
        "        for i in range(y_tokens.shape[0]):\n",
        "            not_pad = y_tokens[i] != tokenizer[\"<PAD>\"]\n",
        "            loss = -y_log_probs[i, not_pad].mean()\n",
        "            out.append(loss.item())\n",
        "\n",
        "    return out\n",
        "def generate_text(model, tokenizer, tokenizer_inv, context=\"<START>\", gen_tokens=10, temperature=0.6):\n",
        "    \"\"\"\n",
        "    Generate a fixed number of tokens using the trained model.\n",
        "    \"\"\"\n",
        "    ## Tokenize the context\n",
        "    init_ids = tokenize(context, pad_to_len=MAX_LEN, tokenizer=tokenizer)\n",
        "    tokens = torch.tensor([init_ids], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(gen_tokens):\n",
        "            ## Get predictions\n",
        "            seq_in = tokens if tokens.size(1) <= MAX_LEN else tokens[:, -MAX_LEN:]\n",
        "            logits, _ = model(seq_in)\n",
        "\n",
        "            ## Focus on the last token's predictions\n",
        "            last_logits = logits[0, -1, :]\n",
        "\n",
        "            ## Apply the softmax to get a probabilities\n",
        "            scaled_logits = last_logits / temperature\n",
        "            probs = F.softmax(scaled_logits, dim=0)\n",
        "\n",
        "            ## Sample from the distribution\n",
        "            next_tok = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            ## Append to the context\n",
        "            tokens = torch.cat([tokens, next_tok.unsqueeze(0)], dim=1)\n",
        "\n",
        "            ## Stop if we generated a STOP token\n",
        "            if next_tok.item() == tokenizer[\"<STOP>\"]:\n",
        "                break\n",
        "\n",
        "    ## Convert back to text\n",
        "    generated_ids = tokens.squeeze(0).tolist()\n",
        "    return decode(generated_ids, tokenizer_inv)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using device : cuda"
      ],
      "metadata": {
        "id": "nhi7rZbssNZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_len=MAX_LEN, dropout=0.1, ff_dim=128):\n",
        "        super(TransformerLM, self).__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, dropout, ff_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "       nn.init.xavier_uniform_(self.token_embedding.weight)\n",
        "       nn.init.xavier_uniform_(self.fc_out.weight)\n",
        "       for layer in self.layers:\n",
        "            layer._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.token_embedding(x)\n",
        "        pos = self.positional_encoding[:, :x.size(1), :]\n",
        "        hidden = self.dropout(emb + pos)\n",
        "        for block in self.layers:\n",
        "            hidden = block(hidden)\n",
        "\n",
        "        logits = self.fc_out(hidden)\n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1, ff_dim=128):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        att = self.attention(x)\n",
        "        att = self.dropout1(att)\n",
        "        x = self.norm1(x + att)\n",
        "        ff_out = self.ff(x)\n",
        "        ff_out = self.dropout2(ff_out)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.proj_q(x)\n",
        "        K = self.proj_k(x)\n",
        "        V = self.proj_v(x)\n",
        "        attn_out = self_attention(x, self.proj_q, self.proj_k, self.proj_v, self.num_heads)\n",
        "        attn_out = self.dropout(attn_out)\n",
        "        out = self.fc_out(attn_out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, tokenizer, tokenizer_inv):\n",
        "    # Using AdamW optimizer with weight decay and learning rate scheduler\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer[\"<PAD>\"])\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_perplexities = []\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        for sample in train_dataset:\n",
        "            token_ids = tokenize(sample, pad_to_len=MAX_LEN, tokenizer=tokenizer)\n",
        "            input_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            input_seq, target_seq = input_tensor[:, :-1], input_tensor[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output_logits, _ = model(input_seq)\n",
        "\n",
        "            loss = loss_fn(output_logits.view(-1, model.fc_out.out_features), target_seq.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_dataset)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for sample in val_dataset:\n",
        "                token_ids = tokenize(sample, pad_to_len=MAX_LEN, tokenizer=tokenizer)\n",
        "                input_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "                input_seq, target_seq = input_tensor[:, :-1], input_tensor[:, 1:]\n",
        "                output_logits, _ = model(input_seq)\n",
        "\n",
        "                val_loss = loss_fn(output_logits.view(-1, model.fc_out.out_features), target_seq.view(-1))\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataset)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Compute and store perplexity for validation set\n",
        "        val_perplexity = np.exp(avg_val_loss)\n",
        "        val_perplexities.append(val_perplexity)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Val Perplexity = {val_perplexity:.2f}\")\n",
        "\n",
        "        # Generate a sample text\n",
        "        generated = generate_text(model, tokenizer, tokenizer_inv, context=\"<START>\", gen_tokens=10)\n",
        "        print(f\"Sample text: {generated}\")\n",
        "\n",
        "        # Update learning rate according to cosine schedule\n",
        "        scheduler.step()\n",
        "\n",
        "    return model, train_losses, val_losses, val_perplexities, tokenizer, tokenizer_inv\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    ## Load and preprocess the training and validation datasets along with the tokenizers\n",
        "    train_dataset, val_dataset, tokenizer, tokenizer_inv = load_and_preprocess_data()\n",
        "\n",
        "    ## Define model hyperparameters\n",
        "    vocab_size = len(tokenizer)\n",
        "    embed_dim = 128\n",
        "    num_heads = 4\n",
        "    num_layers = 4\n",
        "    dropout = 0.1\n",
        "    ff_dim = 256\n",
        "\n",
        "\n",
        "    model = TransformerLM(\n",
        "        vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        ff_dim=ff_dim\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    ## Display the model structure\n",
        "    print(model)\n",
        "\n",
        "    ## Train the model\n",
        "    model, train_losses, val_losses, val_perplexities, tokenizer, tokenizer_inv = train_model(\n",
        "        model, train_dataset, val_dataset, tokenizer, tokenizer_inv\n",
        "    )\n",
        "\n",
        "    ## Plot training and validation losses over epochs\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    ## Save the trained model parameters for later use\n",
        "    torch.save(model.state_dict(), \"transformer_model.pth\")\n",
        "\n",
        "    ## Compute and print the overall validation perplexity\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "    overall_val_perplexity = np.exp(avg_val_loss)\n",
        "    print(f\"\\nDev Perplexity: {overall_val_perplexity}\")\n",
        "\n",
        "    # Code for evaluating test data is commented out below:\n",
        "    # with open(\"/content/shakespear_test.txt\", \"r\") as f:\n",
        "    #     lines_test = f.readlines()\n",
        "    # print(f\"\\nTest perplexity: {}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "MXzHOmZZsSrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model_path, test_file, tokenizer, tokenizer_inv, gen_tokens=10, temperature=0.6):\n",
        "    ## Load the saved model\n",
        "    model = TransformerLM(\n",
        "        vocab_size=len(tokenizer),\n",
        "        embed_dim=128,\n",
        "        num_heads=4,\n",
        "        num_layers=4,\n",
        "        dropout=0.1,\n",
        "        ff_dim=256\n",
        "    ).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    ## Read and process the input from test.txt\n",
        "    with open(test_file, 'r') as f:\n",
        "        test_lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    processed_data = []\n",
        "    for line in test_lines:\n",
        "        tokens = [tokenizer[\"<START>\"]] + \\\n",
        "                 [tokenizer.get(word, tokenizer[\"<PAD>\"]) for word in line.split()] + \\\n",
        "                 [tokenizer[\"<STOP>\"]]\n",
        "        padded = pad_to_length(tokens, MAX_LEN, tokenizer)\n",
        "        processed_data.append(padded)\n",
        "\n",
        "    ## Generate text and calculate perplexity\n",
        "    test_losses = evaluate_losses(processed_data, model, tokenizer)\n",
        "    perplexity = np.exp(np.mean(test_losses))\n",
        "\n",
        "    generated_texts = []\n",
        "    for line in test_lines:\n",
        "        context = ' '.join(line.split()[:10])\n",
        "        generated = generate_text(\n",
        "            model, tokenizer, tokenizer_inv,\n",
        "            context=context,\n",
        "            gen_tokens=gen_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        generated_texts.append(generated)\n",
        "\n",
        "    return generated_texts, perplexity\n",
        "\n",
        "\n",
        "# Example usage\n",
        "model_path = \"transformer_model.pth\"\n",
        "test_file = \"shakespear_test.txt\"\n",
        "_, _, tokenizer, tokenizer_inv = load_and_preprocess_data()\n",
        "generated_texts, ppl = inference(model_path, test_file, tokenizer, tokenizer_inv)\n",
        "\n",
        "## Print the generated text and perplexity\n",
        "print(f\"\\nTest Perplexity: {ppl:.2f}\")\n",
        "print(\"\\nGenerated Examples:\")\n",
        "for i, text in enumerate(generated_texts[:10]):\n",
        "    print(f\"Sample text: {i+1}: {text}\")\n"
      ],
      "metadata": {
        "id": "rrEzAkyisV2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}