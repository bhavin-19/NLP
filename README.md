# ğŸ§  Advanced NLP Pipelines

This repository showcases a collection of advanced NLP projects, including custom tokenization, word embedding models, neural language modeling, sentiment analysis, question answering, and multimodal understanding. All models are implemented using PyTorch and pretrained embeddings (Word2Vec, FastText, GloVe, BERT, etc.).

---

## ğŸ“Œ Table of Contents

- [1. End-to-End NLP Pipeline](#1-end-to-end-nlp-pipeline)
- [2. Aspect-Based Sentiment Analysis & QA](#2-aspect-based-sentiment-analysis--qa)
- [3. Transformer Language Modeling & Multimodal Sarcasm Explanation](#3-transformer-language-modeling--multimodal-sarcasm-explanation)

---

## 1. End-to-End NLP Pipeline

- âœ… Built from scratch:
  - WordPiece Tokenizer
  - Word2Vec (CBOW) Embedding Model
  - MLP-based Neural Language Model
- ğŸ“ˆ Features:
  - Next-word prediction using custom embeddings
  - Multiple architectural variations
  - Loss and embedding visualization (e.g., cosine similarity)

---

## 2. Aspect-Based Sentiment Analysis & QA

- ğŸ¯ Tasks:
  - Aspect Term Extraction
  - Aspect-Based Sentiment Analysis (ABSA)
  - Span-level Question Answering (QA) on SQuAD v2

- ğŸ§  Models:
  - RNN / GRU + FastText / GloVe / BERT for ABSA
  - SpanBERT & SpanBERT-CRF for QA

- ğŸ”§ Highlights:
  - Custom BIO tagging & instance-wise preprocessing
  - Multi-task pipeline with unified training
  - Metric tracking: F1, Accuracy, Exact Match (EM)

---

## 3. Transformer Language Modeling & Multimodal Sarcasm Explanation

- ğŸ“œ Tasks:
  - Shakespearean Language Modeling (Transformer from scratch)
  - Claim Normalization using BART & T5
  - Multimodal Sarcasm Explanation

- ğŸ–¼ï¸ Architecture:
  - Fusion of BART + Vision Transformer (ViT)
  - Custom shared encoder-decoder for text+image input

- ğŸ§ª Evaluation:
  - F1, Accuracy, EM metrics
  - Loss-performance visualization across configurations

---

## ğŸ› ï¸ Setup

```bash
git clone https://github.com/bhavin-19/NLP.git
cd NLP
pip install -r requirements.txt
